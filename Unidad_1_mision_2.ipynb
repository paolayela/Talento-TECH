{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6JJZ3oVTAgi+nJVrK6Hr2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/paolayela/Talento-TECH/blob/main/Unidad_1_mision_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "         MODELO DE CLASIFICACION."
      ],
      "metadata": {
        "id": "M1FLZsCMoKdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#cargar el conjunto de datos\n",
        "iris=load_iris()\n",
        "x,y =iris.data, iris.target\n",
        "\n",
        "#explorando los datos\n",
        "type(iris)\n",
        "iris.keys()\n",
        "iris['data']\n",
        "iris['target']\n",
        "iris['target_names']\n",
        "iris['DESCR']\n",
        "iris['feature_names']\n",
        "\n",
        "#dividir los datos en entrenamiento y prueba\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2, random_state=42)\n",
        "\n",
        "#escalara las caracteristicas para un mejor rendimiento del modelo\n",
        "scaler=StandardScaler()\n",
        "x_train_scaled=scaler.fit_transform(x_train)\n",
        "x_test_scaled=scaler.fit_transform(x_test)\n",
        "\n",
        "#crear una instancia de MLPClassifier\n",
        "mlp_clf=MLPClassifier(hidden_layer_sizes=(100),\n",
        "                      activation='relu',\n",
        "                      solver='adam',\n",
        "                      max_iter=100,\n",
        "                      random_state=42,\n",
        "                      verbose=True\n",
        "                      )\n",
        "\n",
        "#Entrenar el modelo\n",
        "mlp_clf.fit(x_train_scaled, y_train)\n",
        "\n",
        "#Realizar predicciones en el conjunto de prueba\n",
        "y_pred=mlp_clf.predict(x_test_scaled)\n",
        "print(y_test)\n",
        "print(y_pred)\n",
        "\n",
        "#Calcular la precision del modelo\n",
        "accuracy=accuracy_score(y_test,y_pred)\n",
        "print('Precision del modelo:', accuracy)\n"
      ],
      "metadata": {
        "id": "__3l_0ZgAlf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "1439d508-b289-430f-930f-02598ffd4698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.16374923\n",
            "Iteration 2, loss = 1.14090611\n",
            "Iteration 3, loss = 1.11857816\n",
            "Iteration 4, loss = 1.09677375\n",
            "Iteration 5, loss = 1.07547149\n",
            "Iteration 6, loss = 1.05468504\n",
            "Iteration 7, loss = 1.03440573\n",
            "Iteration 8, loss = 1.01462373\n",
            "Iteration 9, loss = 0.99535820\n",
            "Iteration 10, loss = 0.97658926\n",
            "Iteration 11, loss = 0.95833257\n",
            "Iteration 12, loss = 0.94053852\n",
            "Iteration 13, loss = 0.92322169\n",
            "Iteration 14, loss = 0.90638373\n",
            "Iteration 15, loss = 0.89000670\n",
            "Iteration 16, loss = 0.87408395\n",
            "Iteration 17, loss = 0.85861030\n",
            "Iteration 18, loss = 0.84357120\n",
            "Iteration 19, loss = 0.82895431\n",
            "Iteration 20, loss = 0.81476466\n",
            "Iteration 21, loss = 0.80097410\n",
            "Iteration 22, loss = 0.78759157\n",
            "Iteration 23, loss = 0.77460302\n",
            "Iteration 24, loss = 0.76199129\n",
            "Iteration 25, loss = 0.74975377\n",
            "Iteration 26, loss = 0.73786826\n",
            "Iteration 27, loss = 0.72632581\n",
            "Iteration 28, loss = 0.71511871\n",
            "Iteration 29, loss = 0.70424319\n",
            "Iteration 30, loss = 0.69368183\n",
            "Iteration 31, loss = 0.68342580\n",
            "Iteration 32, loss = 0.67346953\n",
            "Iteration 33, loss = 0.66380608\n",
            "Iteration 34, loss = 0.65442161\n",
            "Iteration 35, loss = 0.64531278\n",
            "Iteration 36, loss = 0.63647902\n",
            "Iteration 37, loss = 0.62790342\n",
            "Iteration 38, loss = 0.61957863\n",
            "Iteration 39, loss = 0.61148527\n",
            "Iteration 40, loss = 0.60361466\n",
            "Iteration 41, loss = 0.59597076\n",
            "Iteration 42, loss = 0.58855743\n",
            "Iteration 43, loss = 0.58136358\n",
            "Iteration 44, loss = 0.57437150\n",
            "Iteration 45, loss = 0.56758016\n",
            "Iteration 46, loss = 0.56097952\n",
            "Iteration 47, loss = 0.55455113\n",
            "Iteration 48, loss = 0.54829175\n",
            "Iteration 49, loss = 0.54219766\n",
            "Iteration 50, loss = 0.53626159\n",
            "Iteration 51, loss = 0.53048210\n",
            "Iteration 52, loss = 0.52484566\n",
            "Iteration 53, loss = 0.51933866\n",
            "Iteration 54, loss = 0.51394846\n",
            "Iteration 55, loss = 0.50869354\n",
            "Iteration 56, loss = 0.50356710\n",
            "Iteration 57, loss = 0.49857160\n",
            "Iteration 58, loss = 0.49370008\n",
            "Iteration 59, loss = 0.48894487\n",
            "Iteration 60, loss = 0.48429908\n",
            "Iteration 61, loss = 0.47974886\n",
            "Iteration 62, loss = 0.47529331\n",
            "Iteration 63, loss = 0.47093384\n",
            "Iteration 64, loss = 0.46667067\n",
            "Iteration 65, loss = 0.46249978\n",
            "Iteration 66, loss = 0.45841914\n",
            "Iteration 67, loss = 0.45442291\n",
            "Iteration 68, loss = 0.45050855\n",
            "Iteration 69, loss = 0.44667746\n",
            "Iteration 70, loss = 0.44292801\n",
            "Iteration 71, loss = 0.43926405\n",
            "Iteration 72, loss = 0.43567120\n",
            "Iteration 73, loss = 0.43215951\n",
            "Iteration 74, loss = 0.42871948\n",
            "Iteration 75, loss = 0.42534205\n",
            "Iteration 76, loss = 0.42201917\n",
            "Iteration 77, loss = 0.41875279\n",
            "Iteration 78, loss = 0.41554234\n",
            "Iteration 79, loss = 0.41238463\n",
            "Iteration 80, loss = 0.40928512\n",
            "Iteration 81, loss = 0.40623752\n",
            "Iteration 82, loss = 0.40323632\n",
            "Iteration 83, loss = 0.40027444\n",
            "Iteration 84, loss = 0.39735268\n",
            "Iteration 85, loss = 0.39447038\n",
            "Iteration 86, loss = 0.39162936\n",
            "Iteration 87, loss = 0.38882549\n",
            "Iteration 88, loss = 0.38605547\n",
            "Iteration 89, loss = 0.38331954\n",
            "Iteration 90, loss = 0.38062197\n",
            "Iteration 91, loss = 0.37796519\n",
            "Iteration 92, loss = 0.37534717\n",
            "Iteration 93, loss = 0.37276083\n",
            "Iteration 94, loss = 0.37020149\n",
            "Iteration 95, loss = 0.36766960\n",
            "Iteration 96, loss = 0.36516474\n",
            "Iteration 97, loss = 0.36267665\n",
            "Iteration 98, loss = 0.36021595\n",
            "Iteration 99, loss = 0.35777846\n",
            "Iteration 100, loss = 0.35536449\n",
            "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "[1 0 2 1 2 0 1 2 1 1 2 0 0 0 0 2 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "Precision del modelo: 0.9333333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "        MODELO DE REGRESION"
      ],
      "metadata": {
        "id": "jLIaZ8looYOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#cargar el conjunto de datos\n",
        "hoesing=fetch_california_housing()\n",
        "x,y =hoesing.data, hoesing.target\n",
        "\n",
        "#explorando los datos\n",
        "type(hoesing)\n",
        "iris.keys()\n",
        "iris['data']\n",
        "iris['target']\n",
        "iris['target_names']\n",
        "iris['DESCR']\n",
        "iris['feature_names']\n",
        "\n",
        "#dividir los datos en entrenamiento y prueba\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2, random_state=42)\n",
        "\n",
        "#escalara las caracteristicas para un mejor rendimiento del modelo\n",
        "scaler=StandardScaler()\n",
        "x_train_scaled=scaler.fit_transform(x_train)\n",
        "x_test_scaled=scaler.fit_transform(x_test)\n",
        "\n",
        "#crear una instancia de MLPClassifier\n",
        "mlp_reg=MLPRegressor(hidden_layer_sizes=(100),\n",
        "                      activation='relu',\n",
        "                      solver='adam',\n",
        "                      max_iter=100,\n",
        "                      random_state=42,\n",
        "                      verbose=True\n",
        "                      )\n",
        "\n",
        "#Entrenar el modelo\n",
        "mlp_reg.fit(x_train_scaled, y_train)\n",
        "\n",
        "#Realizar predicciones en el conjunto de prueba\n",
        "y_pred=mlp_reg.predict(x_test_scaled)\n",
        "\n",
        "#Calcular la precision del modelo\n",
        "mse=mean_squared_error(y_test,y_pred)\n",
        "print('Error cuadratico medio del modelo:', mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ash0As-QoXvV",
        "outputId": "0885fee2-15b6-44c0-9339-a643c431f9f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 1.62056447\n",
            "Iteration 2, loss = 0.43678846\n",
            "Iteration 3, loss = 0.32700527\n",
            "Iteration 4, loss = 0.28069940\n",
            "Iteration 5, loss = 0.24814135\n",
            "Iteration 6, loss = 0.22803342\n",
            "Iteration 7, loss = 0.21538976\n",
            "Iteration 8, loss = 0.20787968\n",
            "Iteration 9, loss = 0.20154389\n",
            "Iteration 10, loss = 0.19743378\n",
            "Iteration 11, loss = 0.19321222\n",
            "Iteration 12, loss = 0.18961729\n",
            "Iteration 13, loss = 0.18678600\n",
            "Iteration 14, loss = 0.18383630\n",
            "Iteration 15, loss = 0.18157601\n",
            "Iteration 16, loss = 0.17968086\n",
            "Iteration 17, loss = 0.17775103\n",
            "Iteration 18, loss = 0.17538371\n",
            "Iteration 19, loss = 0.17423819\n",
            "Iteration 20, loss = 0.17407294\n",
            "Iteration 21, loss = 0.17085613\n",
            "Iteration 22, loss = 0.16995187\n",
            "Iteration 23, loss = 0.16823352\n",
            "Iteration 24, loss = 0.16798530\n",
            "Iteration 25, loss = 0.16717835\n",
            "Iteration 26, loss = 0.16566984\n",
            "Iteration 27, loss = 0.16730781\n",
            "Iteration 28, loss = 0.16499618\n",
            "Iteration 29, loss = 0.16261953\n",
            "Iteration 30, loss = 0.16145474\n",
            "Iteration 31, loss = 0.16053131\n",
            "Iteration 32, loss = 0.16002333\n",
            "Iteration 33, loss = 0.16125255\n",
            "Iteration 34, loss = 0.15863582\n",
            "Iteration 35, loss = 0.15835131\n",
            "Iteration 36, loss = 0.15795060\n",
            "Iteration 37, loss = 0.15703766\n",
            "Iteration 38, loss = 0.15707574\n",
            "Iteration 39, loss = 0.15768516\n",
            "Iteration 40, loss = 0.15525691\n",
            "Iteration 41, loss = 0.15632172\n",
            "Iteration 42, loss = 0.15467901\n",
            "Iteration 43, loss = 0.15438915\n",
            "Iteration 44, loss = 0.15556876\n",
            "Iteration 45, loss = 0.15329744\n",
            "Iteration 46, loss = 0.15273177\n",
            "Iteration 47, loss = 0.15307344\n",
            "Iteration 48, loss = 0.15369159\n",
            "Iteration 49, loss = 0.15240098\n",
            "Iteration 50, loss = 0.15143071\n",
            "Iteration 51, loss = 0.15112755\n",
            "Iteration 52, loss = 0.15083169\n",
            "Iteration 53, loss = 0.15061975\n",
            "Iteration 54, loss = 0.17536085\n",
            "Iteration 55, loss = 0.15573187\n",
            "Iteration 56, loss = 0.15049969\n",
            "Iteration 57, loss = 0.14988656\n",
            "Iteration 58, loss = 0.15022315\n",
            "Iteration 59, loss = 0.14961153\n",
            "Iteration 60, loss = 0.14848394\n",
            "Iteration 61, loss = 0.14950369\n",
            "Iteration 62, loss = 0.14843954\n",
            "Iteration 63, loss = 0.14811978\n",
            "Iteration 64, loss = 0.14766317\n",
            "Iteration 65, loss = 0.14793107\n",
            "Iteration 66, loss = 0.14733359\n",
            "Iteration 67, loss = 0.14800560\n",
            "Iteration 68, loss = 0.14665149\n",
            "Iteration 69, loss = 0.14686777\n",
            "Iteration 70, loss = 0.14665771\n",
            "Iteration 71, loss = 0.14612037\n",
            "Iteration 72, loss = 0.14639346\n",
            "Iteration 73, loss = 0.14671723\n",
            "Iteration 74, loss = 0.14576906\n",
            "Iteration 75, loss = 0.15060883\n",
            "Iteration 76, loss = 0.14555955\n",
            "Iteration 77, loss = 0.14514246\n",
            "Iteration 78, loss = 0.14553115\n",
            "Iteration 79, loss = 0.14643439\n",
            "Iteration 80, loss = 0.14501085\n",
            "Iteration 81, loss = 0.14468993\n",
            "Iteration 82, loss = 0.14489975\n",
            "Iteration 83, loss = 0.14516740\n",
            "Iteration 84, loss = 0.14435448\n",
            "Iteration 85, loss = 0.14397719\n",
            "Iteration 86, loss = 0.14633622\n",
            "Iteration 87, loss = 0.14389402\n",
            "Iteration 88, loss = 0.14379916\n",
            "Iteration 89, loss = 0.14337806\n",
            "Iteration 90, loss = 0.14402555\n",
            "Iteration 91, loss = 0.14472118\n",
            "Iteration 92, loss = 0.14661691\n",
            "Iteration 93, loss = 0.15462744\n",
            "Iteration 94, loss = 0.14386535\n",
            "Iteration 95, loss = 0.14275879\n",
            "Iteration 96, loss = 0.14295641\n",
            "Iteration 97, loss = 0.14219076\n",
            "Iteration 98, loss = 0.14180756\n",
            "Iteration 99, loss = 0.14206889\n",
            "Iteration 100, loss = 0.14286992\n",
            "Error cuadratico medio del modelo: 7.037412580000361\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}